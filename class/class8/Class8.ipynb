{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 8 - Parallelism in Julia\n",
    "\n",
    "Today we'll talk a bit about Julia's built-in tecniques for taking advantage of parallelism, as well as Julia's MPI interface and the new parallel linear algebra package that wraps Elemental.\n",
    "\n",
    "## Example: Monte Carlo Simulations\n",
    "\n",
    "One of the many ways that computers have aided science is through simulation.  Sometimes you may not have a closed-form way to access a quantity of interest, and can obtain a good guess through running many simulations with parameters drawn from a distribution, and looking at the average behavior of your model.  This class of methods is known as [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method).  \n",
    "\n",
    "One of the benefits of Monte Carlo methods is that they are often trivially parallelizable, since you can run independent experiments on separate processes, and then aggregate the results in a single round of communication at the end.\n",
    "\n",
    "One of the great uses of Monte Carlo methods is [integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration), which becomes increasingly attractive over high-dimensional domains.  The cannonical example is estimating $\\pi$ by integrating a circle on a square domian.\n",
    "\n",
    "The area of a circle with unit radius is $\\pi r^2 = \\pi$.\n",
    "The area of a square on $[-1, 1]^2$ is 4.  If we place the unit circle in this square, the ratio of their areas is $\\pi/4$.  The idea is that we sample uniformly on this square, and then see what portion of the points lie in the circle.  We know that this ratio should be approximately $\\pi/4$, so re multiply the ratio by 4 to obtain our approximation of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "θ = linspace(0,2π,1000)\n",
    "circ_xs = cos(θ)\n",
    "circ_ys = sin(θ)\n",
    "\n",
    "sq_xs = [ 1, 1,-1,-1, 1]\n",
    "sq_ys = [-1, 1, 1,-1,-1]\n",
    "\n",
    "r_xs = rand(100) * 2 - 1\n",
    "r_ys = rand(100) * 2 - 1\n",
    "\n",
    "plot(circ_xs,circ_ys, \"b-\", sq_xs, sq_ys, \"g-\", r_xs, r_ys, \"r.\")\n",
    "axis([-1.2, 1.2, -1.1, 1.1])\n",
    "xkcd()\n",
    "title(\"Setup\")\n",
    "show()\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function π_monte_carlo(n_samples::Int64)\n",
    "    n_circle = 0\n",
    "    for i=1:n_samples\n",
    "        x = rand() * 2 - 1\n",
    "        y = rand() * 2 - 1\n",
    "        r2 = x^2 + y^2\n",
    "        if r2 <= 1\n",
    "            n_circle += 1\n",
    "        end\n",
    "    end\n",
    "    return (n_circle / n_samples) * 4\n",
    "end\n",
    "\n",
    "@time π_monte_carlo(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = Array(Float64,0)\n",
    "n_pts = 2.^(6:30) # up to 1 billion\n",
    "for n_samples in n_pts\n",
    "    tic()\n",
    "    push!(errors, π_monte_carlo(n_samples) - π)\n",
    "    toc()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "xkcd(false)\n",
    "show()\n",
    "zx = [1, maximum(n_pts)]\n",
    "zy = [0, 0]\n",
    "semilogx(zx, zy ,\"k--\", n_pts, errors, \".r\" )\n",
    "title(\"Error of MC calculation\")\n",
    "show()\n",
    "\n",
    "exp_acc = 1./ √(n_pts)\n",
    "\n",
    "loglog(n_pts, exp_acc, abs(errors'), \".r\")\n",
    "title(\"Error of MC calculation vs. CLT\")\n",
    "show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes us ~15 sec. to estimate $\\pi$ on a billion points. However, this used only one core on the machine.  What if we want to use more?  \n",
    "\n",
    "## Using more than one process\n",
    "\n",
    "If you're starting up Julia in a terminal, you can use\n",
    "```\n",
    "julia -n 4\n",
    "```\n",
    "To indicate that 4 processes are available.  If you're using a IJulia notebook, you can add processes with `addprocs()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@show nprocs()\n",
    "addprocs(3)\n",
    "@show nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(my machine has 4 cores).  Now, Julia will know that it can use up to 4 separate processes, but when you're writing code, you you need to explicitly use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = @spawn randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that all processes have access to functions that you define.  In order to define a function on all processes use the `@everywhere` macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sum_test(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = @spawn sum_test(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function sum_test(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = @spawn sum_test(1,2)\n",
    "fetch(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're loading modules, `using` will load them on all processes, but `include()` will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distributions\n",
    "d = @spawn Exponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2 = fetch(d)\n",
    "rand(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control which process is used with `@spawnat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = @spawnat 2 1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spawn/fetch commands are great for spinning off function evaluations.  If you want something that looks like a parallel for-loop, you can use the `@parallel` macro.  Note that this example does something like a MPI gather. (example from http://docs.julialang.org/en/release-0.4/manual/parallel-computing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nheads = @parallel (+) for i=1:200000000\n",
    "  Int(rand(Bool))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use a parallel for-loop, you should make sure that the inner contents of the loop are independent of each other, since you aren't controlling evaluation order, or which process is doing what.  Note that every process also uses its own copy of data, so something like the following will not work like you might wish it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = rand(100)\n",
    "@parallel for i = 1:100\n",
    "    a[i] = 1\n",
    "end\n",
    "@show a\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a distributed array, you can use the [DistributedArrays](https://github.com/JuliaParallel/DistributedArrays.jl) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Modify the Monte Carlo calculation of $\\pi$ above to use more than one process. (There's more than one way to do this!)\n",
    "* How fast is your modified version compared to the single process version?\n",
    "* Can you add the `@simd` macro to the for-loop?  How does this compare with parallelization?  What happens when you mix parallelization and `@simd`?\n",
    "\n",
    "If you're stuck, there's a nice guide on doing this in Julia [here](http://mathemathinking.com/uncategorized/parallel-monte-carlo-in-julia/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays\n",
    "\n",
    "As mentioned above, if you wish to have multiple processes work on a single array you need to go above an beyond the limitiations of a standard array.  There are two types of arrays you may wish to use for parallel/distributed computations:\n",
    "\n",
    "* [Shared Arrays](http://docs.julialang.org/en/release-0.4/manual/parallel-computing/#shared-arrays) - built into Julia - all processes can access any element of the array\n",
    "* [Distributed Arrays](https://github.com/JuliaParallel/DistributedArrays.jl) - package through JuliaParallel - array elements are distributed over processes.\n",
    "\n",
    "### Shared Arrays\n",
    "\n",
    "[Shared Arrays](http://docs.julialang.org/en/release-0.4/manual/parallel-computing/#shared-arrays) are accessible from all processes and maintain the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = SharedArray(Int, (3,4), init = S -> S[Base.localindexes(S)] = myid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the type of the array elements, the second argument is a tuple of dimensions, the (optional) third argument is an initialization function, and the (optional) fourth argument denotes what processes the Shared Array should be shared on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = SharedArray(Int, (3,4), init = S -> S[indexpids(S):length(procs(S)):length(S)] = myid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = @spawnat 2 S[1] = 7\n",
    "fetch(p)\n",
    "S # displayed by process 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Arrays\n",
    "\n",
    "[Distributed Arrays](https://github.com/JuliaParallel/DistributedArrays.jl) are offered from [Julia Parallel](https://github.com/JuliaParallel) to distribute an array over several processes.  If you've done distributed linear algebra before, this is probably more familiar than the SharedArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using DistributedArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = drandn(5, 5)\n",
    "println(B)\n",
    "A = @DArray [i+j for i = 1:5, j = 1:5]\n",
    "println(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed arrays will change the order of operations, so you should expect to see differences due to floating point errors when you perform operations with DArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = randn(100,100)\n",
    "@show s1 = sum(A)\n",
    "D = distribute(A)\n",
    "@show s2 = sum(D)\n",
    "@show s1 - s2\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "* Can you use linear algebra routines (such as SVD) on SharedArrays?  What about DArrays?  What is the type of the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Julia with MPI\n",
    "\n",
    "Most of what we've considered above is useful for using multiple cores on a single machine.  What if you want to run Julia on a cluster?  Julia provides a [MPI package](https://github.com/JuliaParallel/MPI.jl) that allows you to run Julia scripts with `mpirun` just as you would with C/Fortran binaries.\n",
    "\n",
    "If you haven't used it before, [MPI](https://computing.llnl.gov/tutorials/mpi/) is the Message Passing Interface Standard.  The standard has several implementations - the most popular are [MPICH](https://www.mpich.org/) and [OpenMPI](http://www.open-mpi.org/) (both are open source), which you can get for your system using almost any repository manager.  Corn.stanford has OpenMPI, I have MPICH. In practice, the two are mostly interchangable, although they use slightly different syntax if you use a [hostfile](https://www.open-mpi.org/faq/?category=running).\n",
    "\n",
    "If you haven't used MPI before, it is worth looking at it a bit if you are interested in scientific computing.  The appeal in Julia is that it allows you finer control when writing algorithms that may use several cores or machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example\n",
    "See the script [here](mpi_test.jl)\n",
    "```bash\n",
    "mpirun -np 4 julia mpi_test.jl\n",
    "```\n",
    "(note that if julia is an alias, then mpirun will throw an error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Julia's MPI interface to simply initialize MPI, and then call C/Fortran libraries that use it more extensively.  This is what we see with the Elemental package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Linear Algebra with Elemental\n",
    "\n",
    "[Elemental](https://github.com/elemental/Elemental) is a library for distributed linear algebra primarily written and maintained by [Jack Poulson](http://web.stanford.edu/~poulson/).  It was recently wrapped by Julia Parallel so you can now do your HPC linear algebra from Julia.  Right now you'll need to clone [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl) using `Pkg.clone()` to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Elemental\n",
    "const El = Elemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example from Elemental.jl's tests\n",
    "using Base.Test\n",
    "using Elemental\n",
    "using DistributedArrays\n",
    "\n",
    "A = drandn(50,50)\n",
    "Al = convert(Array, A)\n",
    "B = drandn(50,10)\n",
    "Bl = convert(Array, B)\n",
    "\n",
    "@test Al\\Bl ≈ A\\B\n",
    "@test svdvals(Al) ≈ Elemental.svdvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elemental provides a DistMatrix type, which will have entries distributed over processes similar to a Julia DArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = El.DistMatrix(Float64)\n",
    "El.gaussian!(M, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = El.DistMatrix(Float64)\n",
    "El.ones!(b, 100, 1)\n",
    "x = El.lav(M,b) # Mx = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elemental.jl is still being developed, and a lot of the library's C++ functionality is still not available through the Julia interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* try running one of the [examples](https://github.com/JuliaParallel/MPI.jl/tree/master/examples) in MPI.jl on corn.stanford.\n",
    "* (if you have time) try compiling Elemental.jl on your system (or corn), and see how fast you can do matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
